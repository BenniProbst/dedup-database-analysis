# =============================================================================
# Dedup Database Analysis - GitLab CI/CD Triple Pipeline
# =============================================================================
# Pipeline 1: LaTeX compilation (auto on .tex/.bib/.cls/.bst changes)
# Pipeline 2: C++ compilation + smoke tests (auto on src/cpp/** changes)
# Pipeline 3: DB experiment (MANUAL trigger only -- production DBs!)
#
# SAFETY: Pipeline 3 uses Samba AD lab user (dedup-lab) and isolated
#         lab schemas (dedup_lab). Customer data is NEVER touched.
#         Lab schemas are created before and dropped after each experiment.
#         Automatic cleanup via after_script guarantees no orphaned data.
#
# Cluster: 4 Worker Nodes (Intel N97), Talos v1.11.6, K8s v1.34.0
# Storage: Longhorn (Replica 4, thin-provisioned)
# Runner: GitLab K8s Runner (ID 6, tag: kubernetes)
#
# Required CI/CD Variables (Settings > CI/CD > Variables, MASKED):
#   DEDUP_PG_PASSWORD       - PostgreSQL lab user password
#   DEDUP_CRDB_PASSWORD     - CockroachDB lab user password (often empty)
#   DEDUP_REDIS_PASSWORD    - Redis password (often empty in cluster mode)
#   DEDUP_MARIADB_PASSWORD  - MariaDB lab user password
#   DEDUP_MINIO_PASSWORD    - MinIO lab user password
# =============================================================================

stages:
  - latex
  - docker-build
  - cpp-build
  - cpp-test
  - experiment-build
  - experiment-run
  - experiment-cleanup

variables:
  PROMETHEUS_URL: http://kube-prometheus-stack-prometheus.monitoring.svc.cluster.local:9090
  KAFKA_BOOTSTRAP: kafka-cluster-kafka-bootstrap.kafka.svc.cluster.local:9092
  REPLICA_COUNT: "4"
  LAB_SCHEMA: dedup_lab
  DUP_GRADES: "U0,U50,U90"
  SYSTEMS: "postgresql,cockroachdb,redis,kafka,minio,mariadb,clickhouse"
  NUM_FILES: "500"
  SEED: "42"
  # Docker registry (for optional image builds)
  DEDUP_IMAGE: ${CI_REGISTRY_IMAGE}/dedup-test

# =============================================================================
# PIPELINE 1: LaTeX Compilation (auto-triggered on document changes)
# =============================================================================

latex:compile:
  stage: latex
  tags:
    - kubernetes
  image: texlive/texlive:latest
  before_script:
    - cd docs
  script:
    - |
      echo "=== LaTeX Compilation: Triple-Pass Build ==="
      pdflatex -interaction=nonstopmode -halt-on-error doku.tex || true
      bibtex doku || true
      pdflatex -interaction=nonstopmode -halt-on-error doku.tex || true
      pdflatex -interaction=nonstopmode -halt-on-error doku.tex
      ls -lh doku.pdf 2>/dev/null && echo "PDF generated successfully" || echo "ERROR: PDF not generated"
      echo "=== Warnings ==="
      grep -c "Warning" doku.log 2>/dev/null && grep "Warning" doku.log | head -10 || echo "None"
      echo "=== Undefined References ==="
      grep "undefined" doku.log | head -5 || echo "None"
  artifacts:
    paths:
      - docs/doku.pdf
      - docs/doku.log
    expire_in: 30 days
    when: always
  rules:
    - if: $CI_PIPELINE_SOURCE == "push" || $CI_PIPELINE_SOURCE == "web" || $CI_PIPELINE_SOURCE == "api"
      changes:
        - "docs/**/*.tex"
        - "docs/**/*.bib"
        - "docs/**/*.cls"
        - "docs/**/*.bst"
        - "docs/Makefile"
    - if: $CI_PIPELINE_SOURCE == "web" || $CI_PIPELINE_SOURCE == "api"

# =============================================================================
# Docker Image Build (optional, manual -- uses Kaniko for K8s runners)
# =============================================================================

docker:build:
  stage: docker-build
  tags:
    - kubernetes
  image:
    name: gcr.io/kaniko-project/executor:v1.23.2-debug
    entrypoint: [""]
  script:
    - |
      echo "=== Building Docker image: ${DEDUP_IMAGE}:${CI_COMMIT_SHORT_SHA} ==="
      mkdir -p /kaniko/.docker
      echo "{\"auths\":{\"${CI_REGISTRY}\":{\"auth\":\"$(printf '%s:%s' "${CI_REGISTRY_USER}" "${CI_REGISTRY_PASSWORD}" | base64)\"}}}" > /kaniko/.docker/config.json
      /kaniko/executor \
        --context "${CI_PROJECT_DIR}" \
        --dockerfile "${CI_PROJECT_DIR}/Dockerfile" \
        --destination "${DEDUP_IMAGE}:${CI_COMMIT_SHORT_SHA}" \
        --destination "${DEDUP_IMAGE}:latest" \
        --cache=true \
        --cache-repo "${DEDUP_IMAGE}/cache"
      echo "=== Image pushed: ${DEDUP_IMAGE}:${CI_COMMIT_SHORT_SHA} ==="
  rules:
    - changes:
        - Dockerfile
        - .dockerignore
        - "src/cpp/**"
      when: manual
      allow_failure: true

# =============================================================================
# PIPELINE 2: C++ Build + Tests (auto on src/cpp/** changes)
# =============================================================================

.cpp-default: &cpp-default
  tags:
    - kubernetes
  image: gcc:14-bookworm
  before_script:
    - |
      apt-get update -qq
      apt-get install -y -qq --no-install-recommends \
        cmake libpq-dev libcurl4-openssl-dev \
        libhiredis-dev librdkafka-dev nlohmann-json3-dev \
        libmariadb-dev \
        && rm -rf /var/lib/apt/lists/*

cpp:build:
  <<: *cpp-default
  stage: cpp-build
  timeout: 30 minutes
  script:
    - |
      echo "=== C++ Build: dedup-integration-test v${CI_COMMIT_SHORT_SHA} ==="
      echo "Compiler: $(gcc --version | head -1)"
      echo "CMake: $(cmake --version | head -1)"

      mkdir -p build && cd build
      cmake ../src/cpp -DCMAKE_BUILD_TYPE=Release

      echo "=== Building dedup-test ==="
      make -j$(nproc) dedup-test 2>&1

      echo "=== Building dedup-smoke-test ==="
      make -j$(nproc) dedup-smoke-test 2>&1

      echo "=== Build artifacts ==="
      ls -lh dedup-test dedup-smoke-test
      file dedup-test
  artifacts:
    paths:
      - build/dedup-test
      - build/dedup-smoke-test
    expire_in: 30 days
  rules:
    - if: $CI_PIPELINE_SOURCE == "web" || $CI_PIPELINE_SOURCE == "api"
    - if: $CI_PIPELINE_SOURCE == "push"
      changes:
        - "src/cpp/**/*.cpp"
        - "src/cpp/**/*.hpp"
        - "src/cpp/**/*.h"
        - "src/cpp/CMakeLists.txt"
        - ".gitlab-ci.yml"

cpp:smoke-test:
  <<: *cpp-default
  stage: cpp-test
  timeout: 20 minutes
  needs: ["cpp:build"]
  script:
    - |
      echo "=== Smoke Test: SHA-256 + Dataset Generator + Dry-Run ==="
      chmod +x build/dedup-smoke-test

      # 1. Generate test datasets (100 files per grade, 16K fixed, seed=42)
      echo "--- Step 1: Generate datasets ---"
      build/dedup-smoke-test --generate-data \
        --data-dir /tmp/datasets \
        --num-files 100 \
        --file-size 16384 \
        --seed 42 \
        --dry-run --verbose 2>&1

      # Verify generated files (structure: data_dir/{payload_type}/{grade}/)
      echo "--- Dataset verification ---"
      echo "Payload type directories:"
      ls -1 /tmp/datasets/
      for pt_dir in /tmp/datasets/*/; do
        PT=$(basename "${pt_dir}")
        for grade in U0 U50 U90; do
          COUNT=$(find "${pt_dir}${grade}" -name '*.dat' 2>/dev/null | wc -l)
          SIZE=$(du -sh "${pt_dir}${grade}/" 2>/dev/null | cut -f1)
          echo "  ${PT}/${grade}: ${COUNT} files, ${SIZE}"
        done
      done

      # 2. Check for duplicates using SHA-256 (per payload type)
      echo "--- Step 2: Verify duplication levels ---"
      for pt_dir in /tmp/datasets/*/; do
        PT=$(basename "${pt_dir}")
        for grade in U0 U50 U90; do
          TOTAL=$(find "${pt_dir}${grade}" -name '*.dat' 2>/dev/null | wc -l)
          UNIQUE=$(find "${pt_dir}${grade}" -name '*.dat' -exec sha256sum {} + 2>/dev/null | awk '{print $1}' | sort -u | wc -l)
          if [ "$TOTAL" -gt 0 ]; then
            DUP_PCT=$(awk "BEGIN {printf \"%.1f\", (1 - ${UNIQUE}/${TOTAL}) * 100}")
          else
            DUP_PCT="0.0"
          fi
          echo "  ${PT}/${grade}: ${TOTAL} total, ${UNIQUE} unique, ${DUP_PCT}% duplicates"
        done
      done

      echo "=== Smoke Test PASSED ==="
  rules:
    - if: $CI_PIPELINE_SOURCE == "web" || $CI_PIPELINE_SOURCE == "api"
    - if: $CI_PIPELINE_SOURCE == "push"
      changes:
        - "src/cpp/**/*.cpp"
        - "src/cpp/**/*.hpp"
        - "src/cpp/**/*.h"
        - "src/cpp/CMakeLists.txt"
        - ".gitlab-ci.yml"

cpp:full-dry-test:
  <<: *cpp-default
  stage: cpp-test
  timeout: 30 minutes
  needs: ["cpp:build"]
  script:
    - |
      echo "=== Full Dry Test: All Systems x All Grades ==="
      chmod +x build/dedup-smoke-test

      # Generate datasets + run full experiment simulation in one invocation
      # (--generate-data generates data, then proceeds to experiment)
      set +e
      build/dedup-smoke-test --generate-data \
        --data-dir /tmp/datasets \
        --results-dir results \
        --num-files 50 \
        --seed 42 \
        --dry-run --verbose 2>&1
      EXIT_CODE=$?
      set -e
      echo "=== Binary exit code: ${EXIT_CODE} ==="

      if [ "$EXIT_CODE" -ne 0 ]; then
        echo "ERROR: dedup-smoke-test exited with code ${EXIT_CODE}"
        echo "--- dmesg (last 10 lines, check for OOM/segfault) ---"
        dmesg 2>/dev/null | tail -10 || true
        exit ${EXIT_CODE}
      fi

      echo "--- Results ---"
      ls -la results/*.json 2>/dev/null || echo "No result files"
      cat results/combined_results.json 2>/dev/null | head -100

      echo "--- Help output ---"
      build/dedup-smoke-test --help

      echo "=== Full Dry Test PASSED ==="
  artifacts:
    paths:
      - results/
    expire_in: 7 days
  rules:
    - if: $CI_PIPELINE_SOURCE == "web" || $CI_PIPELINE_SOURCE == "api"
    - if: $CI_PIPELINE_SOURCE == "push"
      changes:
        - "src/cpp/**/*.cpp"
        - "src/cpp/**/*.hpp"
        - "src/cpp/**/*.h"
        - "src/cpp/CMakeLists.txt"
        - ".gitlab-ci.yml"

# =============================================================================
# PIPELINE 3: Real DB Experiment (MANUAL trigger only)
# =============================================================================
# SAFETY CONCEPT:
#   - Uses Samba AD lab user (dedup-lab) with restricted permissions
#   - All data goes into isolated lab schemas (dedup_lab / dedup-lab prefix)
#   - Lab schemas are CREATED before experiment and DROPPED after
#   - Customer/production data is NEVER read, modified, or deleted
#   - after_script GUARANTEES cleanup even on failure/timeout
#   - Each DB system uses its own isolation mechanism:
#     PostgreSQL/CockroachDB: CREATE SCHEMA dedup_lab (separate from public)
#     Redis: Key prefix dedup:* (no SELECT in cluster mode)
#     Kafka: Topic prefix dedup-lab-* (separate from production topics)
#     MinIO: Bucket dedup-lab-* (separate from production buckets)
#     MariaDB/ClickHouse: CREATE DATABASE dedup_lab (separate database)
# =============================================================================

# Step 1: Build the binary for the experiment (self-contained, no dependency
# on Pipeline 2 artifacts which may have expired or not exist)
experiment:build:
  <<: *cpp-default
  stage: experiment-build
  script:
    - |
      echo "=== Experiment Build: dedup-test v${CI_COMMIT_SHORT_SHA} ==="
      echo "Compiler: $(gcc --version | head -1)"
      mkdir -p build && cd build
      cmake ../src/cpp -DCMAKE_BUILD_TYPE=Release
      make -j$(nproc) dedup-test 2>&1
      echo "=== Binary ready ==="
      ls -lh dedup-test
      file dedup-test
  artifacts:
    paths:
      - build/dedup-test
    expire_in: 7 days
  rules:
    - when: manual
      allow_failure: true

# Step 2: Run the actual experiment against production databases
# IMPORTANT: after_script runs cleanup EVEN IF the main script fails/times out
experiment:run:
  <<: *cpp-default
  stage: experiment-run
  needs: ["experiment:build"]
  timeout: 4 hours
  variables:
    GIT_STRATEGY: clone
  script:
    - |
      echo "================================================================="
      echo "  REAL EXPERIMENT: Dedup Database Analysis"
      echo "  WARNING: Connects to PRODUCTION databases via lab schemas!"
      echo "  Lab user: dedup-lab (Samba AD)"
      echo "  Lab schema: ${LAB_SCHEMA}"
      echo "================================================================="
      echo ""
      echo "Systems: ${SYSTEMS}"
      echo "Grades: ${DUP_GRADES}"
      echo "Replica count: ${REPLICA_COUNT}"
      echo "Prometheus: ${PROMETHEUS_URL}"
      echo ""

      chmod +x build/dedup-test

      # --- Generate experiment config from CI variables ---
      echo "=== Generating config.json from CI variables ==="
      cat > /tmp/dedup-config.json << ENDCONF
      {
        "lab_user": "dedup-lab",
        "lab_schema": "${LAB_SCHEMA}",
        "data_dir": "/tmp/datasets",
        "results_dir": "results",
        "replica_count": ${REPLICA_COUNT},
        "dry_run": false,
        "databases": [
          {
            "system": "postgresql",
            "host": "postgres-lb.databases.svc.cluster.local",
            "port": 5432,
            "user": "dedup-lab",
            "password": "${DEDUP_PG_PASSWORD}",
            "database": "postgres",
            "lab_schema": "${LAB_SCHEMA}",
            "pvc_name": "data-postgres-ha-0",
            "k8s_namespace": "databases"
          },
          {
            "system": "cockroachdb",
            "host": "cockroachdb-public.cockroach-operator-system.svc.cluster.local",
            "port": 26257,
            "user": "dedup_lab",
            "password": "${DEDUP_CRDB_PASSWORD}",
            "database": "${LAB_SCHEMA}",
            "lab_schema": "${LAB_SCHEMA}",
            "pvc_name": "datadir-cockroachdb-0",
            "k8s_namespace": "cockroach-operator-system"
          },
          {
            "system": "redis",
            "host": "redis-cluster.redis.svc.cluster.local",
            "port": 6379,
            "password": "${DEDUP_REDIS_PASSWORD}",
            "pvc_name": "data-redis-cluster-0",
            "k8s_namespace": "redis"
          },
          {
            "system": "kafka",
            "host": "${KAFKA_BOOTSTRAP}",
            "port": 9092,
            "lab_schema": "dedup-lab",
            "pvc_name": "data-kafka-cluster-broker-0",
            "k8s_namespace": "kafka"
          },
          {
            "system": "minio",
            "host": "minio-lb.minio.svc.cluster.local",
            "port": 9000,
            "user": "dedup-lab",
            "password": "${DEDUP_MINIO_PASSWORD}",
            "lab_schema": "dedup-lab",
            "pvc_name": "",
            "k8s_namespace": "minio"
          },
          {
            "system": "mariadb",
            "host": "mariadb.databases.svc.cluster.local",
            "port": 3306,
            "user": "dedup-lab",
            "password": "${DEDUP_MARIADB_PASSWORD}",
            "database": "${LAB_SCHEMA}",
            "lab_schema": "${LAB_SCHEMA}",
            "pvc_name": "",
            "k8s_namespace": "databases"
          },
          {
            "system": "clickhouse",
            "host": "clickhouse.databases.svc.cluster.local",
            "port": 8123,
            "lab_schema": "${LAB_SCHEMA}",
            "pvc_name": "",
            "k8s_namespace": "databases"
          }
        ],
        "dup_grades": ["U0", "U50", "U90"],
        "prometheus": {
          "url": "${PROMETHEUS_URL}"
        },
        "metrics_trace": {
          "kafka_bootstrap": "${KAFKA_BOOTSTRAP}",
          "kafka_topic": "dedup-lab-metrics",
          "events_topic": "dedup-lab-events",
          "sample_interval_ms": 100,
          "enabled": true
        },
        "git_export": {
          "remote_name": "origin",
          "branch": "${CI_COMMIT_REF_NAME}",
          "auto_push": true,
          "ssl_verify": false
        }
      }
      ENDCONF
      echo "Config generated: /tmp/dedup-config.json"

      # --- Set up git for results export (push before cleanup) ---
      echo "=== Setting up git for results export ==="
      git config user.email "dedup-ci@comdare.de"
      git config user.name "Dedup CI Pipeline"
      git remote set-url origin "https://gitlab-ci-token:${CI_JOB_TOKEN}@${CI_SERVER_HOST}/${CI_PROJECT_PATH}.git" || true

      # --- Step 1: Generate test datasets ---
      echo "=== Step 1: Generate datasets (${NUM_FILES} files x 3 grades) ==="
      build/dedup-test --generate-data \
        --data-dir /tmp/datasets \
        --num-files "${NUM_FILES}" \
        --seed "${SEED}" \
        --verbose 2>&1

      # Verify dataset sizes
      echo "=== Dataset summary ==="
      du -sh /tmp/datasets/*/

      # --- Step 2: Run experiment with config ---
      echo "=== Step 2: Run experiment ==="
      build/dedup-test \
        --config /tmp/dedup-config.json \
        --data-dir /tmp/datasets \
        --systems "${SYSTEMS}" \
        --grades "${DUP_GRADES}" \
        --lab-schema "${LAB_SCHEMA}" \
        --results-dir results \
        --verbose 2>&1

      echo "=== Step 3: Results ==="
      ls -la results/*.json 2>/dev/null || echo "No result files"
      cat results/combined_results.json 2>/dev/null | head -200

      echo "=== Experiment completed successfully ==="

  # CRITICAL: after_script runs EVEN IF main script fails or times out
  # This guarantees lab schemas are always cleaned up
  after_script:
    - |
      echo "================================================================="
      echo "  AUTO-CLEANUP: Dropping all lab schemas (runs on success+failure)"
      echo "================================================================="
      if [ -x build/dedup-test ]; then
        # Reinstall runtime deps (after_script runs in fresh shell)
        apt-get update -qq 2>/dev/null
        apt-get install -y -qq --no-install-recommends libpq5 libcurl4 \
          libhiredis1.1.0 librdkafka1 libmariadb3 2>/dev/null || true
        build/dedup-test \
          --cleanup-only \
          --systems "${SYSTEMS}" \
          --lab-schema "${LAB_SCHEMA}" \
          --verbose 2>&1
        echo "=== Auto-cleanup complete ==="
      else
        echo "WARNING: dedup-test binary not available for cleanup"
        echo "Use experiment:cleanup job to manually clean up lab schemas"
      fi
  artifacts:
    paths:
      - results/
    expire_in: 90 days
  rules:
    - when: manual
      allow_failure: true

# Step 3: Manual cleanup fallback -- in case after_script cleanup failed
# or experiment:run was never triggered but schemas exist from a previous run
experiment:cleanup:
  <<: *cpp-default
  stage: experiment-cleanup
  needs:
    - job: experiment:build
      artifacts: true
    - job: experiment:run
      optional: true
  script:
    - |
      echo "================================================================="
      echo "  MANUAL CLEANUP: Dropping all lab schemas"
      echo "  This removes ONLY ${LAB_SCHEMA} data, NOT customer data!"
      echo "================================================================="

      chmod +x build/dedup-test

      build/dedup-test \
        --cleanup-only \
        --systems "${SYSTEMS}" \
        --lab-schema "${LAB_SCHEMA}" \
        --verbose 2>&1

      echo "=== Manual cleanup complete: all lab schemas dropped ==="
  rules:
    - when: manual
      allow_failure: true
