# COMDARE Multi-Platform CI/CD Template v5.4.1 (BS-3040)
# v5.4.1: upload-to-minio uses python3 S3v4 (no external mc download needed)
# v5.4.0: K8s timeout 15min (was 10min, build needs ~9min with apt-get overhead)
# v5.3.0: aggregate+upload via rules:when:always + stage-ordering, 10min timeout on K8s
# v5.2.0: Unique build dirs + coverage per BM OS, corrected artifact handling
# v5.1.0: Split BM into Debian + Ubuntu (incompatible OS, separate runner pools)
# v5.0.0: ALL build jobs in single stage (parallel execution)
#
# === Runner Architecture (12 total, 10 online, 2 paused) ===
# Parallelization: 7 independent execution lanes in single build stage
#
# Lane 1: K8s (Docker)        -- kubernetes tag -- pve1/node3/node4 Talos VMs
# Lane 2: BM-Debian (Shell)   -- debian+x86_64  -- pve1 (pve2 paused/experiment)
# Lane 3: BM-Ubuntu (Shell)   -- ubuntu+x86_64  -- node3, node4
# Lane 4: macOS x86 (Shell)   -- macos+x86_64   -- node5 (independent)
# Lane 5: macOS ARM (Shell)   -- macos+arm64    -- node6 (independent)
# Lane 6: Linux ARM64 (Shell) -- arm64+linux    -- node7 (independent)
# Lane 7: Linux RISC-V (Shell)-- riscv64+linux  -- node8 (independent)
#
# XOR: K8s runner XOR BM runner per physical host (pve1/pve2/node3/node4)
# Exotic nodes (5-8) have no XOR constraint -- fully independent
# Phase 1: K8s + BM = MANDATORY, Exotic = allow_failure

stages:
  - build
  - aggregate
  - upload
  - experiment-build
  - experiment-preflight
  - experiment-run
  - experiment-cleanup

variables:
  GCC_VERSION: "14"
  BUILD_TYPE: "Debug"
  INSERTION_MODE: "both"  # blob, native, or both
  REPEAT_DB: ""  # Set to e.g. "mariadb" to repeat only that DB

# ============================================================
# Stage 1: ALL Builds (7 parallel execution lanes)
# ============================================================

# --- Lane 1: K8s Build (Docker executor, MANDATORY) ---
build-k8s:
  stage: build
  timeout: 15 minutes
  image: gcc:${GCC_VERSION}
  tags:
    - kubernetes
  before_script:
    - '(while true; do sleep 25; echo "[keepalive $(date -u +%H:%M:%S)]"; done) &'
    - KEEPALIVE_PID=$!
    - echo 'Acquire::ForceIPv4 "true";' > /etc/apt/apt.conf.d/99force-ipv4
    - sed -i 's|http://deb.debian.org|http://cdn-fastly.deb.debian.org|g' /etc/apt/sources.list.d/*.sources 2>/dev/null || true
    - apt-get update -qq 2>&1 || echo "[WARN] apt install failed"
    - apt-get install -y -qq cmake ninja-build git python3-pip 2>&1 || echo "[WARN] apt install failed"
    - pip install gcovr --break-system-packages -q
    - git config --global url."http://gitlab-ci-token:${CI_JOB_TOKEN}@gitlab-webservice-default.gitlab.svc:8181/".insteadOf "https://gitlab.comdare.de/"
  script:
    - echo "=== COMDARE CI v5.4.1 -- K8s (gcc:${GCC_VERSION}, x86_64) ==="
    - test -f CMakeLists.txt || { echo "No CMakeLists.txt -- skipping"; kill $KEEPALIVE_PID 2>/dev/null || true; exit 0; }
    - cmake -B build-k8s -G Ninja -DCMAKE_BUILD_TYPE=${BUILD_TYPE} -DBUILD_TESTS=ON -DCMAKE_CXX_FLAGS="--coverage -std=c++23" -DCMAKE_C_FLAGS=--coverage -DCMAKE_EXE_LINKER_FLAGS=--coverage
    - cmake --build build-k8s -j4
    - cd build-k8s && ctest --output-on-failure -j$(nproc) || true
    - cd $CI_PROJECT_DIR
    - kill $KEEPALIVE_PID 2>/dev/null || true
    - sleep 1
    - gcovr --root . --filter 'include/' --filter 'src/' --exclude 'build-k8s/_deps' --print-summary --xml coverage-k8s.xml || true
  coverage: '/lines:\s+(\d+\.\d+)%/'
  artifacts:
    paths:
      - coverage-k8s.xml
      - build-k8s/
    reports:
      coverage_report:
        coverage_format: cobertura
        path: coverage-k8s.xml
    expire_in: 30 days
    when: always

# --- Lane 2: Debian x86_64 (pve1, pve2) -- MANDATORY ---
build-debian-x86:
  stage: build
  tags:
    - debian
    - x86_64
  before_script:
    - 'echo "=== COMDARE CI v5.4.1 -- Debian x86_64 ($(uname -m)) ==="'
    - 'command -v git >/dev/null || { echo "ERROR: git not found. Run BS-3049 bootstrap."; exit 1; }'
    - 'command -v cmake >/dev/null || { echo "ERROR: cmake not found. Run BS-3049 bootstrap."; exit 1; }'
    - 'command -v gcc >/dev/null || { echo "ERROR: gcc not found. Run BS-3049 bootstrap."; exit 1; }'
    - 'echo "Toolchain: gcc $(gcc -dumpversion 2>/dev/null), cmake $(cmake --version 2>/dev/null | head -1)"'
  script:
    - test -f CMakeLists.txt || { echo "No CMakeLists.txt -- skipping"; exit 0; }
    - cmake -B build-debian -G Ninja -DCMAKE_BUILD_TYPE=${BUILD_TYPE} -DBUILD_TESTS=ON -DCMAKE_CXX_FLAGS="--coverage -std=c++23" -DCMAKE_C_FLAGS=--coverage -DCMAKE_EXE_LINKER_FLAGS=--coverage
    - cmake --build build-debian -j $(nproc)
    - cd build-debian && ctest --output-on-failure -j$(nproc) || true
    - cd $CI_PROJECT_DIR
    - 'command -v gcovr >/dev/null && gcovr --root . --filter include/ --filter src/ --exclude build-debian/_deps --print-summary --xml coverage-debian.xml || echo "gcovr not found -- skipping coverage"'
  coverage: '/lines:\s+(\d+\.\d+)%/'
  artifacts:
    paths:
      - coverage-debian.xml
      - build-debian/
    reports:
      coverage_report:
        coverage_format: cobertura
        path: coverage-debian.xml
    expire_in: 30 days
    when: always

# --- Lane 3: Ubuntu x86_64 (node3, node4) -- MANDATORY ---
build-ubuntu-x86:
  stage: build
  tags:
    - ubuntu
    - x86_64
  before_script:
    - 'echo "=== COMDARE CI v5.4.1 -- Ubuntu x86_64 ($(uname -m)) ==="'
    - 'command -v git >/dev/null || { echo "ERROR: git not found. Run BS-3049 bootstrap."; exit 1; }'
    - 'command -v cmake >/dev/null || { echo "ERROR: cmake not found. Run BS-3049 bootstrap."; exit 1; }'
    - 'command -v gcc >/dev/null || { echo "ERROR: gcc not found. Run BS-3049 bootstrap."; exit 1; }'
    - 'echo "Toolchain: gcc $(gcc -dumpversion 2>/dev/null), cmake $(cmake --version 2>/dev/null | head -1)"'
  script:
    - test -f CMakeLists.txt || { echo "No CMakeLists.txt -- skipping"; exit 0; }
    - cmake -B build-ubuntu -G Ninja -DCMAKE_BUILD_TYPE=${BUILD_TYPE} -DBUILD_TESTS=ON -DCMAKE_CXX_FLAGS="--coverage -std=c++23" -DCMAKE_C_FLAGS=--coverage -DCMAKE_EXE_LINKER_FLAGS=--coverage
    - cmake --build build-ubuntu -j $(nproc)
    - cd build-ubuntu && ctest --output-on-failure -j$(nproc) || true
    - cd $CI_PROJECT_DIR
    - 'command -v gcovr >/dev/null && gcovr --root . --filter include/ --filter src/ --exclude build-ubuntu/_deps --print-summary --xml coverage-ubuntu.xml || echo "gcovr not found -- skipping coverage"'
  coverage: '/lines:\s+(\d+\.\d+)%/'
  artifacts:
    paths:
      - coverage-ubuntu.xml
      - build-ubuntu/
    reports:
      coverage_report:
        coverage_format: cobertura
        path: coverage-ubuntu.xml
    expire_in: 30 days
    when: always

# --- Lanes 4-7: Exotic Platforms (Shell executor, SOFT-FAIL Phase 1) ---
.exotic-base:
  stage: build
  allow_failure: true
  before_script:
    - 'echo "=== COMDARE CI v5.4.1 -- Exotic ($(uname -s) $(uname -m)) ==="'
    - 'command -v git >/dev/null || { echo "WARN: git not found -- skipping"; exit 0; }'
    - 'command -v cmake >/dev/null || { echo "WARN: cmake not found -- skipping"; exit 0; }'
  script:
    - test -f CMakeLists.txt || { echo "No CMakeLists.txt -- skipping"; exit 0; }
    - 'GEN=""; command -v ninja >/dev/null && GEN="-G Ninja"'
    - cmake -B build-exotic $GEN -DCMAKE_BUILD_TYPE=${BUILD_TYPE} -DBUILD_TESTS=ON -DCMAKE_CXX_STANDARD=23
    - cmake --build build-exotic -j $(nproc 2>/dev/null || sysctl -n hw.ncpu 2>/dev/null || echo 2)
    - cd build-exotic && ctest --output-on-failure || true
    - cd $CI_PROJECT_DIR
  artifacts:
    paths:
      - build-exotic/
    expire_in: 30 days
    when: always

# Lane 4: macOS x86_64 (node5)
build-macos-x86:
  extends: .exotic-base
  tags:
    - macos
    - x86_64

# Lane 5: macOS ARM64 (node6)
build-macos-arm:
  extends: .exotic-base
  tags:
    - macos
    - arm64

# Lane 6: Linux ARM64 (node7)
build-linux-arm64:
  extends: .exotic-base
  tags:
    - arm64
    - linux

# Lane 7: Linux RISC-V (node8)
build-linux-riscv:
  extends: .exotic-base
  tags:
    - riscv64
    - linux

# ============================================================
# Stage 2: Aggregate Results
# ============================================================
aggregate-results:
  stage: aggregate
  image: alpine:latest
  tags:
    - kubernetes
  rules:
    - when: always
  script:
    - echo "=== COMDARE CI v5.4.1 -- Aggregate Results ==="
    - echo "Platform build results:"
    - '[ -f coverage-k8s.xml ] && echo "  K8s Docker (x86_64):  OK" || echo "  K8s Docker (x86_64):  MISSING"'
    - '[ -f coverage-debian.xml ] && echo "  Debian BM (x86_64):   OK" || echo "  Debian BM (x86_64):   MISSING"'
    - '[ -f coverage-ubuntu.xml ] && echo "  Ubuntu BM (x86_64):   OK" || echo "  Ubuntu BM (x86_64):   MISSING"'
    - '[ -d build-exotic ] && echo "  Exotic platforms:      artifacts present" || echo "  Exotic platforms:      no artifacts"'
    - echo "Pipeline $CI_PIPELINE_ID complete on $(date -u)"
  artifacts:
    paths:
      - coverage-k8s.xml
      - coverage-debian.xml
      - coverage-ubuntu.xml
    expire_in: 30 days
    when: always

# ============================================================
# Stage 3: Upload Artifacts to MinIO (buildsystem-artifacts)
# RBMM Path Hierarchy: {project}/{platform}/{arch}/{runner_class}/{pipeline_id}/
# ============================================================
upload-to-minio:
  stage: upload
  image: python:3.12-alpine
  tags:
    - kubernetes
  script:
    - echo "=== COMDARE CI v5.4.1 -- MinIO Artifact Upload (S3v4 native) ==="
    - echo "RBMM Hierarchy -- {project}/{platform}/{arch}/{runner_class}/{pipeline_id}/"
    - |
      python3 << 'PYEOF'
      import hashlib, hmac, datetime, os, sys, urllib.request, urllib.parse, urllib.error

      def get_signing_key(secret, datestamp, region, service):
          k = hmac.new(('AWS4' + secret).encode(), datestamp.encode(), hashlib.sha256).digest()
          k = hmac.new(k, region.encode(), hashlib.sha256).digest()
          k = hmac.new(k, service.encode(), hashlib.sha256).digest()
          return hmac.new(k, b'aws4_request', hashlib.sha256).digest()

      def s3_put(filepath, bucket, obj_key, endpoint, access, secret):
          with open(filepath, 'rb') as f:
              payload = f.read()
          parsed = urllib.parse.urlparse(endpoint)
          host = parsed.netloc
          scheme = parsed.scheme
          t = datetime.datetime.utcnow()
          amz_date = t.strftime('%Y%m%dT%H%M%SZ')
          ds = t.strftime('%Y%m%d')
          region, service = 'us-east-1', 's3'
          uri = '/' + bucket + '/' + obj_key
          payload_hash = hashlib.sha256(payload).hexdigest()
          headers_str = f'host:{host}\nx-amz-content-sha256:{payload_hash}\nx-amz-date:{amz_date}\n'
          signed_h = 'host;x-amz-content-sha256;x-amz-date'
          creq = f'PUT\n{uri}\n\n{headers_str}\n{signed_h}\n{payload_hash}'
          scope = f'{ds}/{region}/{service}/aws4_request'
          sts = f'AWS4-HMAC-SHA256\n{amz_date}\n{scope}\n{hashlib.sha256(creq.encode()).hexdigest()}'
          sig = hmac.new(get_signing_key(secret, ds, region, service), sts.encode(), hashlib.sha256).hexdigest()
          auth = f'AWS4-HMAC-SHA256 Credential={access}/{scope}, SignedHeaders={signed_h}, Signature={sig}'
          url = f'{scheme}://{host}{uri}'
          req = urllib.request.Request(url, data=payload, method='PUT')
          req.add_header('Authorization', auth)
          req.add_header('x-amz-date', amz_date)
          req.add_header('x-amz-content-sha256', payload_hash)
          req.add_header('Content-Type', 'application/xml')
          try:
              resp = urllib.request.urlopen(req, timeout=30)
              return resp.status
          except urllib.error.HTTPError as e:
              print(f'  HTTP {e.code}: {e.read().decode()[:100]}', file=sys.stderr)
              return e.code

      endpoint = os.environ.get('MINIO_ENDPOINT', '')
      access = os.environ.get('MINIO_ACCESS_KEY', '')
      secret = os.environ.get('MINIO_SECRET_KEY', '')
      proj = os.environ.get('CI_PROJECT_PATH', '')
      pid = os.environ.get('CI_PIPELINE_ID', '')
      bucket = 'buildsystem-artifacts'
      uploads = [
          ('coverage-k8s.xml', f'{proj}/linux/x86_64/k8s/{pid}/coverage/coverage-k8s.xml'),
          ('coverage-debian.xml', f'{proj}/linux/x86_64/debian/{pid}/coverage/coverage-debian.xml'),
          ('coverage-ubuntu.xml', f'{proj}/linux/x86_64/ubuntu/{pid}/coverage/coverage-ubuntu.xml'),
      ]
      ok = 0
      for local, key in uploads:
          if os.path.exists(local):
              code = s3_put(local, bucket, key, endpoint, access, secret)
              status = 'OK' if code == 200 else f'FAIL({code})'
              print(f'  {status} {key}')
              if code == 200: ok += 1
          else:
              print(f'  SKIP {local} (not found)')
      print(f'\nUpload complete: {ok}/{len(uploads)} files for pipeline {pid}')
      PYEOF
  allow_failure: true
  rules:
    - if: '$MINIO_ENDPOINT && $MINIO_ACCESS_KEY && $MINIO_SECRET_KEY'
      when: always
    - when: never

# =============================================================================
# PIPELINE 2: Real DB Experiment (sequential per-DB, 3 repetitions each)
# =============================================================================
# Runner: k8s-runner-4 (ID 17) on dedicated node talos-say-ls6
#   - Tag: "experiment" (only tag, run_untagged=false)
#   - Node selector: kubernetes.io/hostname=talos-say-ls6
#   - Toleration: experiment=dedicated:NoSchedule
# Safety: Samba AD lab user (dedup-lab), isolated lab schemas, cleanup guaranteed
# =============================================================================

.experiment-default: &experiment-default
  image: gcc:14-bookworm
  tags:
    - experiment
  before_script:
    - |
      # ForceIPv4 removed (DNS on experiment node returns only IPv6 for CDN)
      sed -i 's|http://deb.debian.org|http://cdn-fastly.deb.debian.org|g' /etc/apt/sources.list.d/*.sources 2>/dev/null || true
      echo "[DEBUG] apt update..."; apt-get update -qq 2>&1 || echo "[WARN] apt update failed"
      apt-get install -y -qq --no-install-recommends \
        libpq-dev libhiredis-dev librdkafka-dev libcurl4-openssl-dev \
        libmariadb-dev nlohmann-json3-dev cmake ninja-build nfs-common curl \
        2>&1 || echo "[WARN] apt install failed"
      echo "Build-tools ready: GCC $(gcc -dumpversion) + CMake + Ninja"

experiment:build:
  <<: *experiment-default
  stage: experiment-build
  needs: []
  script:
    - |
      echo "=== Experiment Build: dedup-test v${CI_COMMIT_SHORT_SHA} ==="
      mkdir -p build && cd build
      cmake ../src/cpp -DCMAKE_BUILD_TYPE=Release
      make -j4 dedup-test 2>&1
      echo "=== Binary ready ==="
      ls -lh dedup-test
      file dedup-test
  artifacts:
    paths:
      - build/dedup-test
    expire_in: 7 days
  rules:
    - if: $CI_PIPELINE_SOURCE == "web" || $CI_PIPELINE_SOURCE == "api"
    - if: $CI_PIPELINE_SOURCE == "push"
      changes:
        - "src/cpp/**/*.cpp"
        - "src/cpp/**/*.hpp"
        - "src/cpp/**/*.h"
        - "src/cpp/CMakeLists.txt"
        - ".gitlab-ci.yml"

experiment:preflight:
  tags:
    - experiment
  image: gcc:14-bookworm
  stage: experiment-preflight
  needs: ["experiment:build"]
  timeout: 10 minutes
  retry:
    max: 2
    when: [runner_system_failure, script_failure, job_execution_timeout]
  script:
    - |
      echo "=== Pre-Flight: Checking production DBs + K8s API + NFS ==="
      echo "(Experiment DBs like MariaDB/ClickHouse/Redis may be scaled to 0 -- checked per-job)"
      PASS=0; FAIL=0
      tcp_check() {
        local host=$1 port=$2
        timeout 5 bash -c "echo > /dev/tcp/${host}/${port}" 2>/dev/null
      }
      check() {
        if eval "$2" 2>/dev/null; then
          echo "  OK  $1"; PASS=$((PASS+1))
        else
          echo "  FAIL $1"; FAIL=$((FAIL+1))
        fi
      }
      # Production DBs (MUST be running)
      check "PostgreSQL"   "tcp_check postgres-lb.databases.svc.cluster.local 5432"
      check "CockroachDB"  "tcp_check cockroachdb-public.cockroach-operator-system.svc.cluster.local 26257"
      check "MinIO"        "tcp_check minio-lb.minio.svc.cluster.local 9000"
      # K8s API (needed for per-DB scaling)
      check "K8s API"      "tcp_check kubernetes.default.svc 443"
      # NFS (needed for checkpoints + real-world datasets)
      check "NFS"          "tcp_check 10.0.110.184 2049"
      # Experiment DBs (informational -- may be at 0 replicas, NOT blocking)
      echo "--- Experiment DBs (info only, not blocking) ---"
      for db_info in "MariaDB:mariadb.databases.svc.cluster.local:3306" \
                     "ClickHouse:clickhouse.databases.svc.cluster.local:8123" \
                     "Redis:redis-cluster.redis.svc.cluster.local:6379" \
                     "Kafka:kafka-cluster-kafka-bootstrap.kafka.svc.cluster.local:9092"; do
        name=$(echo "$db_info" | cut -d: -f1)
        host=$(echo "$db_info" | cut -d: -f2)
        port=$(echo "$db_info" | cut -d: -f3)
        if tcp_check "$host" "$port"; then
          echo "  UP   $name"
        else
          echo "  DOWN $name (will be scaled up by its job)"
        fi
      done
      echo "=== Result: ${PASS}/5 production checks passed, ${FAIL}/5 failed ==="
      [ "$FAIL" -eq 0 ] || exit 1
  rules:
    - if: $CI_PIPELINE_SOURCE == "web" || $CI_PIPELINE_SOURCE == "api"
    - if: $CI_PIPELINE_SOURCE == "push"
      changes:
        - "src/cpp/**/*.cpp"
        - "src/cpp/**/*.hpp"
        - "src/cpp/**/*.h"
        - "src/cpp/CMakeLists.txt"
        - ".gitlab-ci.yml"

.experiment-per-db: &experiment-per-db
  <<: *experiment-default
  stage: experiment-run
  timeout: 6 hours
  # resource_group removed (deadlock on retry; needs-chain + concurrent=1 ensures serialization)
  retry:
    max: 2
    when: [runner_system_failure, unknown_failure]
  script:
    - |
      # === DEBUG: Capture all output to artifact file (workaround for broken trace storage) ===
      mkdir -p results/${DB_SYSTEM:-unknown}
      exec > >(tee results/${DB_SYSTEM:-unknown}/debug.log) 2>&1
      echo "[DEBUG] Output capture started at $(date -u)"
      echo "[DEBUG] Image: $(cat /etc/os-release 2>/dev/null | head -1 || echo unknown)"
      echo "[DEBUG] pwd=$(pwd) whoami=$(whoami)"
      echo "[DEBUG] build/dedup-test exists: $(ls -la build/dedup-test 2>&1)"
      echo "[DEBUG] ldd build/dedup-test:"
      ldd build/dedup-test 2>&1 || echo "[DEBUG] ldd failed"
      echo "[DEBUG] === END INITIAL DEBUG ==="

      # Keepalive heartbeat (prevents stuck_or_timeout_failure)
      (while true; do sleep 60; echo "[keepalive $(date -u +%H:%M:%S)]"; done) &
      KEEPALIVE_PID=$!
      trap "kill $KEEPALIVE_PID 2>/dev/null" EXIT

      # === K8s API helpers for per-DB scaling ===
      APISERVER=https://kubernetes.default.svc
      SA_TOKEN=$(cat /var/run/secrets/kubernetes.io/serviceaccount/token 2>/dev/null || echo "")
      SA_CACERT=/var/run/secrets/kubernetes.io/serviceaccount/ca.crt

      k8s_scale_sts() {
        local ns=$1 name=$2 replicas=$3
        echo "[Scale] ${ns}/${name} -> ${replicas} replicas"
        curl -sk --cacert $SA_CACERT -H "Authorization: Bearer $SA_TOKEN" \
          -H "Content-Type: application/merge-patch+json" \
          -X PATCH "${APISERVER}/apis/apps/v1/namespaces/${ns}/statefulsets/${name}/scale" \
          -d "{\"spec\":{\"replicas\":${replicas}}}" 2>/dev/null | head -c 200 || true
        echo ""
      }

      k8s_scale_kafka() {
        local ns=$1 pool=$2 replicas=$3
        echo "[Scale] kafka/${pool} -> ${replicas} replicas"
        curl -sk --cacert $SA_CACERT -H "Authorization: Bearer $SA_TOKEN" \
          -H "Content-Type: application/merge-patch+json" \
          -X PATCH "${APISERVER}/apis/kafka.strimzi.io/v1beta2/namespaces/${ns}/kafkanodepools/${pool}" \
          -d "{\"spec\":{\"replicas\":${replicas}}}" 2>/dev/null | head -c 200 || true
        echo ""
      }

      wait_for_db() {
        local host=$1 port=$2 timeout=${3:-180}
        echo "[Wait] Waiting for ${host}:${port} (max ${timeout}s)..."
        for i in $(seq 1 $timeout); do
          if timeout 2 bash -c "echo > /dev/tcp/${host}/${port}" 2>/dev/null; then
            echo "[Wait] ${host}:${port} ready after ${i}s"
            return 0
          fi
          sleep 1
        done
        echo "[WARN] Timeout waiting for ${host}:${port} after ${timeout}s"
        return 1
      }

      scale_down_experiment_dbs() {
        echo "[Scale] All experiment DBs stay running (no scale-down)"
      }

      echo "================================================================="
      echo "  EXPERIMENT: ${DB_SYSTEM} (3 repetitions)"
      echo "  Lab user: dedup-lab | Lab schema: dedup_lab"
      echo "  Payload types: ${PAYLOAD_TYPES}"
      echo "  DB scaling: type=${DB_TYPE} ns=${DB_NAMESPACE}"
      echo "  Checkpoint: NFS-persistent"
      echo "================================================================="

      # NFS is mounted via Runner-4 ConfigMap [[runners.kubernetes.volumes.nfs]]
      # mount_path: /datasets/real-world, server: 10.0.110.184, path: /
      if [ -d /datasets/real-world ]; then
        echo "[NFS] Volume mounted at /datasets/real-world (K8s NFS volume)"
      else
        echo "[NFS] WARNING: NFS volume not available -- checkpoints local-only"
        mkdir -p /datasets/real-world
      fi
      mkdir -p /datasets/real-world/checkpoints/${DB_SYSTEM}

      # Copy real-world datasets from NFS to working directory if available
      if [ -d /datasets/real-world ] && [ "$(ls -A /datasets/real-world 2>/dev/null | grep -v checkpoints)" ]; then
        echo "[NFS] Copying real-world datasets to /tmp/datasets/"
        for dtype in bank_transactions text_corpus numeric_dataset github_events; do
          if [ -d "/datasets/real-world/${dtype}" ]; then
            mkdir -p "/tmp/datasets/${dtype}"
            cp -r "/datasets/real-world/${dtype}/"* "/tmp/datasets/${dtype}/" 2>/dev/null || true
            echo "[NFS] Copied ${dtype}: $(find /tmp/datasets/${dtype} -type f 2>/dev/null | wc -l) files"
          fi
        done
      else
        echo "[NFS] No real-world datasets on NFS -- using synthetic data only"
      fi

      # All DBs stay running -- just verify connectivity
      echo "[Scale] All DBs stay running (no scale up/down)"
      wait_for_db "${DB_HOST}" "${DB_PORT}" 120

      chmod +x build/dedup-test
      OVERALL_EXIT=0

      for RUN in 1 2 3; do
        echo ""
        echo "===== ${DB_SYSTEM}: Run ${RUN}/3 ====="
        SEED=$((42 + RUN))
        mkdir -p results/${DB_SYSTEM}/run-${RUN}

        # Check NFS-persistent checkpoint (survives OOM/pod restarts)
        if [ -f "/datasets/real-world/checkpoints/${DB_SYSTEM}/${DB_SYSTEM}_run${RUN}.checkpoint" ]; then
          echo "[Checkpoint] Run ${RUN} already completed -- skipping"
          continue
        fi

        echo "--- Step 1: Generate data (seed=${SEED}) ---"
        build/dedup-test \
          --generate-data \
          --insertion-mode "${INSERTION_MODE}" \
          --payload-types "${PAYLOAD_TYPES}" \
          --config src/cpp/config.example.json \
          --data-dir /tmp/datasets \
          --num-files 500 \
          --file-size 524288 \
          --seed ${SEED} \
          --verbose 2>&1 || echo "[WARN] Data generation exit code: $?"

        REPEAT_ARG=""
        if [ -n "${REPEAT_DB}" ]; then
          REPEAT_ARG="--repeat-db ${REPEAT_DB}"
        fi
        echo "--- Step 2: Run experiment (${DB_SYSTEM}, run ${RUN}) ---"
        build/dedup-test \
          --config src/cpp/config.example.json \
          --systems "${DB_SYSTEM}" \
          --insertion-mode "${INSERTION_MODE}" \
          --payload-types "${PAYLOAD_TYPES}" \
          --data-dir /tmp/datasets \
          --results-dir "results/${DB_SYSTEM}/run-${RUN}" \
          --checkpoint-dir "/datasets/real-world/checkpoints/${DB_SYSTEM}" \
          --run-id ${RUN} \
          --max-retries 3 \
          ${REPEAT_ARG} \
          --verbose 2>&1 || {
            echo "WARNING: Run ${RUN} exited with code $?"
            OVERALL_EXIT=1
          }

        echo "===== Run ${RUN}/3 complete ====="
        ls -la results/${DB_SYSTEM}/run-${RUN}/ 2>/dev/null || true
      done

      echo ""
      echo "=== All 3 runs complete for ${DB_SYSTEM} ==="
      echo "=== Overall exit: ${OVERALL_EXIT} ==="

      # All DBs stay running after test (no scale-down)
      echo "[Scale] ${DB_SYSTEM} stays running after test"

      if ls results/${DB_SYSTEM}/run-*/combined_results.json 2>/dev/null; then
        echo "Results files found"
      fi

      # NFS volume auto-unmounted by K8s (no manual umount needed)
      exit ${OVERALL_EXIT}
  after_script:
    - |
      echo "=== after_script: No scale-down (all DBs stay running) ==="
      # NFS volume auto-unmounted by K8s (no manual umount needed)
  artifacts:
    paths:
      - results/
    expire_in: 365 days
    when: always
  rules:
    - if: $CI_PIPELINE_SOURCE == "web" || $CI_PIPELINE_SOURCE == "api"
    - if: $CI_PIPELINE_SOURCE == "push"
      changes:
        - "src/cpp/**/*.cpp"
        - "src/cpp/**/*.hpp"
        - "src/cpp/**/*.h"
        - "src/cpp/CMakeLists.txt"
        - ".gitlab-ci.yml"

experiment:postgresql:
  <<: *experiment-per-db
  needs: ["experiment:minio", "experiment:build"]
  variables:
    DB_SYSTEM: "postgresql"
    DB_TYPE: "none"
    DB_HOST: "postgres-lb.databases.svc.cluster.local"
    DB_PORT: "5432"
    PAYLOAD_TYPES: "random_binary,structured_json,text_document,uuid_keys,jsonb_documents,bank_transactions,text_corpus,numeric_dataset"

experiment:cockroachdb:
  <<: *experiment-per-db
  needs: ["experiment:postgresql", "experiment:build"]
  variables:
    DB_SYSTEM: "cockroachdb"
    DB_TYPE: "none"
    DB_HOST: "cockroachdb-public.cockroach-operator-system.svc.cluster.local"
    DB_PORT: "26257"
    PAYLOAD_TYPES: "random_binary,structured_json,text_document,uuid_keys,jsonb_documents,bank_transactions,text_corpus,numeric_dataset"

experiment:mariadb:
  <<: *experiment-per-db
  needs: ["experiment:preflight", "experiment:build"]
  variables:
    DB_SYSTEM: "mariadb"
    DB_TYPE: "statefulset"
    DB_NAMESPACE: "databases"
    DB_STATEFULSET: "mariadb"
    DB_REPLICAS: "1"
    DB_HOST: "mariadb.databases.svc.cluster.local"
    DB_PORT: "3306"
    PAYLOAD_TYPES: "random_binary,structured_json,text_document,uuid_keys,jsonb_documents,bank_transactions,text_corpus,numeric_dataset"

experiment:clickhouse:
  <<: *experiment-per-db
  needs: ["experiment:mariadb", "experiment:build"]
  variables:
    DB_SYSTEM: "clickhouse"
    DB_TYPE: "statefulset"
    DB_NAMESPACE: "databases"
    DB_STATEFULSET: "clickhouse"
    DB_REPLICAS: "1"
    DB_HOST: "clickhouse.databases.svc.cluster.local"
    DB_PORT: "8123"
    PAYLOAD_TYPES: "random_binary,structured_json,text_document,uuid_keys,jsonb_documents,bank_transactions,text_corpus,numeric_dataset"

experiment:redis:
  <<: *experiment-per-db
  needs: ["experiment:clickhouse", "experiment:build"]
  variables:
    DB_SYSTEM: "redis"
    DB_TYPE: "statefulset"
    DB_NAMESPACE: "redis"
    DB_STATEFULSET: "redis-cluster"
    DB_REPLICAS: "3"
    DB_HOST: "redis-cluster.redis.svc.cluster.local"
    DB_PORT: "6379"
    PAYLOAD_TYPES: "random_binary,structured_json,text_document,uuid_keys,jsonb_documents"

experiment:kafka:
  <<: *experiment-per-db
  needs: ["experiment:redis", "experiment:build"]
  variables:
    DB_SYSTEM: "kafka"
    DB_TYPE: "kafka"
    DB_NAMESPACE: "kafka"
    DB_REPLICAS: "1"
    DB_HOST: "kafka-cluster-kafka-bootstrap.kafka.svc.cluster.local"
    DB_PORT: "9092"
    PAYLOAD_TYPES: "random_binary,structured_json,text_document,uuid_keys,jsonb_documents"

experiment:minio:
  <<: *experiment-per-db
  needs: ["experiment:kafka", "experiment:build"]
  variables:
    DB_SYSTEM: "minio"
    DB_TYPE: "none"
    DB_HOST: "minio-lb.minio.svc.cluster.local"
    DB_PORT: "9000"
    PAYLOAD_TYPES: "random_binary,structured_json,text_document,uuid_keys,jsonb_documents"

experiment:run-all:
  stage: experiment-run
  tags:
    - experiment
  image: alpine:latest
  when: manual
  needs: ["experiment:build"]
  script:
    - echo "Manual trigger for full experiment cascade"
    - echo "Use individual DB jobs instead (they chain automatically)"
  rules:
    - if: $CI_PIPELINE_SOURCE == "web" || $CI_PIPELINE_SOURCE == "api"
      when: manual

experiment:upload-results:
  stage: experiment-cleanup
  tags:
    - experiment
  image: gcc:14-bookworm
  needs:
    - job: experiment:postgresql
      artifacts: true
    - job: experiment:cockroachdb
      artifacts: true
    - job: experiment:mariadb
      artifacts: true
    - job: experiment:clickhouse
      artifacts: true
    - job: experiment:redis
      artifacts: true
    - job: experiment:kafka
      artifacts: true
    - job: experiment:minio
      artifacts: true
  script:
    - |
      echo "=== Experiment Results Upload to measurement_results/ ==="
      apt-get update -qq 2>&1 | tail -1
      apt-get install -y -qq git 2>&1 | tail -1

      # Configure git for commit
      git config user.email "dedup-lab@comdare.de"
      git config user.name "Experiment Pipeline"
      git config http.sslVerify false

      # Use CI token for push
      PUSH_URL="http://gitlab-ci-token:${CI_JOB_TOKEN}@gitlab-webservice-default.gitlab.svc:8181/${CI_PROJECT_PATH}.git"
      git remote set-url origin "$PUSH_URL" 2>/dev/null || git remote add origin "$PUSH_URL"

      # Fetch and checkout the branch
      git fetch origin "${CI_COMMIT_REF_NAME}" --depth=1
      git checkout -B "${CI_COMMIT_REF_NAME}" "origin/${CI_COMMIT_REF_NAME}"

      # Create measurement_results directory with timestamp
      TIMESTAMP=$(date -u +%Y%m%d-%H%M%S)
      RESULT_DIR="measurement_results/pipeline-${CI_PIPELINE_ID}_${TIMESTAMP}"
      mkdir -p "${RESULT_DIR}"

      # Copy all experiment results
      if [ -d results ]; then
        cp -r results/* "${RESULT_DIR}/" 2>/dev/null || true
        echo "Copied results to ${RESULT_DIR}/"
        find "${RESULT_DIR}" -type f | head -50
      else
        echo "WARNING: No results directory found"
      fi

      # Create summary file
      cat > "${RESULT_DIR}/pipeline_info.json" << INFOEOF
      {
        "pipeline_id": ${CI_PIPELINE_ID},
        "commit_sha": "${CI_COMMIT_SHA}",
        "commit_short": "${CI_COMMIT_SHORT_SHA}",
        "branch": "${CI_COMMIT_REF_NAME}",
        "timestamp": "${TIMESTAMP}",
        "runner": "k8s-runner-4 (talos-say-ls6)",
        "databases": ["postgresql","cockroachdb","mariadb","clickhouse","redis","kafka","minio"]
      }
      INFOEOF

      # Commit and push
      git add measurement_results/
      if git diff --cached --quiet; then
        echo "No new results to commit"
      else
        COMMIT_MSG="experiment: results pipeline #${CI_PIPELINE_ID} (${TIMESTAMP})"
        git commit -m "${COMMIT_MSG}"
        git push origin "${CI_COMMIT_REF_NAME}" || {
          echo "WARNING: Push failed, retrying with rebase..."
          git pull --rebase origin "${CI_COMMIT_REF_NAME}" && \
          git push origin "${CI_COMMIT_REF_NAME}" || \
          echo "ERROR: Could not push results. Check GitLab permissions."
        }
        echo "=== Results committed and pushed ==="
        echo "Commit: $(git rev-parse --short HEAD)"
        echo "Directory: ${RESULT_DIR}"
      fi
  allow_failure: true
  rules:
    - if: $CI_PIPELINE_SOURCE == "web" || $CI_PIPELINE_SOURCE == "api"
    - if: $CI_PIPELINE_SOURCE == "push"
      changes:
        - "src/cpp/**/*.cpp"
        - "src/cpp/**/*.hpp"
        - "src/cpp/**/*.h"
        - "src/cpp/CMakeLists.txt"
        - ".gitlab-ci.yml"

experiment:cleanup:
  stage: experiment-cleanup
  tags:
    - experiment
  image: alpine:latest
  when: manual
  needs: []
  script:
    - echo "=== Post-Experiment Cleanup ==="
    - echo "Manual cleanup -- verify results before running"
  rules:
    - if: $CI_PIPELINE_SOURCE == "web" || $CI_PIPELINE_SOURCE == "api"
      when: manual
