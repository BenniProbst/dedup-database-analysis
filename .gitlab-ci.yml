# COMDARE Multi-Platform CI/CD Template v5.4.1 (BS-3040)
# v5.4.1: upload-to-minio uses python3 S3v4 (no external mc download needed)
# v5.4.0: K8s timeout 15min (was 10min, build needs ~9min with apt-get overhead)
# v5.3.0: aggregate+upload via rules:when:always + stage-ordering, 10min timeout on K8s
# v5.2.0: Unique build dirs + coverage per BM OS, corrected artifact handling
# v5.1.0: Split BM into Debian + Ubuntu (incompatible OS, separate runner pools)
# v5.0.0: ALL build jobs in single stage (parallel execution)
#
# === Runner Architecture (12 total, 10 online, 2 paused) ===
# Parallelization: 7 independent execution lanes in single build stage
#
# Lane 1: K8s (Docker)        — kubernetes tag — pve1/node3/node4 Talos VMs
# Lane 2: BM-Debian (Shell)   — debian+x86_64  — pve1 (pve2 paused/experiment)
# Lane 3: BM-Ubuntu (Shell)   — ubuntu+x86_64  — node3, node4
# Lane 4: macOS x86 (Shell)   — macos+x86_64   — node5 (independent)
# Lane 5: macOS ARM (Shell)   — macos+arm64    — node6 (independent)
# Lane 6: Linux ARM64 (Shell) — arm64+linux    — node7 (independent)
# Lane 7: Linux RISC-V (Shell)— riscv64+linux  — node8 (independent)
#
# XOR: K8s runner XOR BM runner per physical host (pve1/pve2/node3/node4)
# Exotic nodes (5-8) have no XOR constraint — fully independent
# Phase 1: K8s + BM = MANDATORY, Exotic = allow_failure

stages:
  - build
  - aggregate
  - upload
  - experiment-build
  - experiment-preflight
  - experiment-run
  - experiment-cleanup

variables:
  GCC_VERSION: "14"
  BUILD_TYPE: "Debug"

# ============================================================
# Stage 1: ALL Builds (7 parallel execution lanes)
# ============================================================

# --- Lane 1: K8s Build (Docker executor, MANDATORY) ---
build-k8s:
  stage: build
  timeout: 15 minutes
  image: gcc:${GCC_VERSION}
  tags:
    - kubernetes
  before_script:
    - '(while true; do sleep 25; echo "[keepalive $(date -u +%H:%M:%S)]"; done) &'
    - KEEPALIVE_PID=$!
    - apt-get update -qq && apt-get install -y -qq cmake ninja-build git python3-pip >/dev/null 2>&1
    - pip install gcovr --break-system-packages -q
    - git config --global url."http://gitlab-ci-token:${CI_JOB_TOKEN}@gitlab-webservice-default.gitlab.svc:8181/".insteadOf "https://gitlab.comdare.de/"
  script:
    - echo "=== COMDARE CI v5.4.1 — K8s (gcc:${GCC_VERSION}, x86_64) ==="
    - test -f CMakeLists.txt || { echo "No CMakeLists.txt — skipping"; kill $KEEPALIVE_PID 2>/dev/null || true; exit 0; }
    - cmake -B build-k8s -G Ninja -DCMAKE_BUILD_TYPE=${BUILD_TYPE} -DBUILD_TESTS=ON -DCMAKE_CXX_FLAGS="--coverage -std=c++23" -DCMAKE_C_FLAGS=--coverage -DCMAKE_EXE_LINKER_FLAGS=--coverage
    - cmake --build build-k8s -j $(nproc)
    - cd build-k8s && ctest --output-on-failure -j$(nproc) || true
    - cd $CI_PROJECT_DIR
    - kill $KEEPALIVE_PID 2>/dev/null || true
    - sleep 1
    - gcovr --root . --filter 'include/' --filter 'src/' --exclude 'build-k8s/_deps' --print-summary --xml coverage-k8s.xml || true
  coverage: '/lines:\s+(\d+\.\d+)%/'
  artifacts:
    paths:
      - coverage-k8s.xml
      - build-k8s/
    reports:
      coverage_report:
        coverage_format: cobertura
        path: coverage-k8s.xml
    expire_in: 30 days
    when: always

# --- Lane 2: Debian x86_64 (pve1, pve2) — MANDATORY ---
build-debian-x86:
  stage: build
  tags:
    - debian
    - x86_64
  before_script:
    - 'echo "=== COMDARE CI v5.4.1 — Debian x86_64 ($(uname -m)) ==="'
    - 'command -v git >/dev/null || { echo "ERROR: git not found. Run BS-3049 bootstrap."; exit 1; }'
    - 'command -v cmake >/dev/null || { echo "ERROR: cmake not found. Run BS-3049 bootstrap."; exit 1; }'
    - 'command -v gcc >/dev/null || { echo "ERROR: gcc not found. Run BS-3049 bootstrap."; exit 1; }'
    - 'echo "Toolchain: gcc $(gcc -dumpversion 2>/dev/null), cmake $(cmake --version 2>/dev/null | head -1)"'
  script:
    - test -f CMakeLists.txt || { echo "No CMakeLists.txt — skipping"; exit 0; }
    - cmake -B build-debian -G Ninja -DCMAKE_BUILD_TYPE=${BUILD_TYPE} -DBUILD_TESTS=ON -DCMAKE_CXX_FLAGS="--coverage -std=c++23" -DCMAKE_C_FLAGS=--coverage -DCMAKE_EXE_LINKER_FLAGS=--coverage
    - cmake --build build-debian -j $(nproc)
    - cd build-debian && ctest --output-on-failure -j$(nproc) || true
    - cd $CI_PROJECT_DIR
    - 'command -v gcovr >/dev/null && gcovr --root . --filter include/ --filter src/ --exclude build-debian/_deps --print-summary --xml coverage-debian.xml || echo "gcovr not found — skipping coverage"'
  coverage: '/lines:\s+(\d+\.\d+)%/'
  artifacts:
    paths:
      - coverage-debian.xml
      - build-debian/
    reports:
      coverage_report:
        coverage_format: cobertura
        path: coverage-debian.xml
    expire_in: 30 days
    when: always

# --- Lane 3: Ubuntu x86_64 (node3, node4) — MANDATORY ---
build-ubuntu-x86:
  stage: build
  tags:
    - ubuntu
    - x86_64
  before_script:
    - 'echo "=== COMDARE CI v5.4.1 — Ubuntu x86_64 ($(uname -m)) ==="'
    - 'command -v git >/dev/null || { echo "ERROR: git not found. Run BS-3049 bootstrap."; exit 1; }'
    - 'command -v cmake >/dev/null || { echo "ERROR: cmake not found. Run BS-3049 bootstrap."; exit 1; }'
    - 'command -v gcc >/dev/null || { echo "ERROR: gcc not found. Run BS-3049 bootstrap."; exit 1; }'
    - 'echo "Toolchain: gcc $(gcc -dumpversion 2>/dev/null), cmake $(cmake --version 2>/dev/null | head -1)"'
  script:
    - test -f CMakeLists.txt || { echo "No CMakeLists.txt — skipping"; exit 0; }
    - cmake -B build-ubuntu -G Ninja -DCMAKE_BUILD_TYPE=${BUILD_TYPE} -DBUILD_TESTS=ON -DCMAKE_CXX_FLAGS="--coverage -std=c++23" -DCMAKE_C_FLAGS=--coverage -DCMAKE_EXE_LINKER_FLAGS=--coverage
    - cmake --build build-ubuntu -j $(nproc)
    - cd build-ubuntu && ctest --output-on-failure -j$(nproc) || true
    - cd $CI_PROJECT_DIR
    - 'command -v gcovr >/dev/null && gcovr --root . --filter include/ --filter src/ --exclude build-ubuntu/_deps --print-summary --xml coverage-ubuntu.xml || echo "gcovr not found — skipping coverage"'
  coverage: '/lines:\s+(\d+\.\d+)%/'
  artifacts:
    paths:
      - coverage-ubuntu.xml
      - build-ubuntu/
    reports:
      coverage_report:
        coverage_format: cobertura
        path: coverage-ubuntu.xml
    expire_in: 30 days
    when: always

# --- Lanes 4-7: Exotic Platforms (Shell executor, SOFT-FAIL Phase 1) ---
.exotic-base:
  stage: build
  allow_failure: true
  before_script:
    - 'echo "=== COMDARE CI v5.4.1 — Exotic ($(uname -s) $(uname -m)) ==="'
    - 'command -v git >/dev/null || { echo "WARN: git not found — skipping"; exit 0; }'
    - 'command -v cmake >/dev/null || { echo "WARN: cmake not found — skipping"; exit 0; }'
  script:
    - test -f CMakeLists.txt || { echo "No CMakeLists.txt — skipping"; exit 0; }
    - 'GEN=""; command -v ninja >/dev/null && GEN="-G Ninja"'
    - cmake -B build-exotic $GEN -DCMAKE_BUILD_TYPE=${BUILD_TYPE} -DBUILD_TESTS=ON -DCMAKE_CXX_STANDARD=23
    - cmake --build build-exotic -j $(nproc 2>/dev/null || sysctl -n hw.ncpu 2>/dev/null || echo 2)
    - cd build-exotic && ctest --output-on-failure || true
    - cd $CI_PROJECT_DIR
  artifacts:
    paths:
      - build-exotic/
    expire_in: 30 days
    when: always

# Lane 4: macOS x86_64 (node5)
build-macos-x86:
  extends: .exotic-base
  tags:
    - macos
    - x86_64

# Lane 5: macOS ARM64 (node6)
build-macos-arm:
  extends: .exotic-base
  tags:
    - macos
    - arm64

# Lane 6: Linux ARM64 (node7)
build-linux-arm64:
  extends: .exotic-base
  tags:
    - arm64
    - linux

# Lane 7: Linux RISC-V (node8)
build-linux-riscv:
  extends: .exotic-base
  tags:
    - riscv64
    - linux

# ============================================================
# Stage 2: Aggregate Results
# ============================================================
aggregate-results:
  stage: aggregate
  image: alpine:latest
  tags:
    - kubernetes
  rules:
    - when: always
  script:
    - echo "=== COMDARE CI v5.4.1 — Aggregate Results ==="
    - echo "Platform build results:"
    - '[ -f coverage-k8s.xml ] && echo "  K8s Docker (x86_64):  OK" || echo "  K8s Docker (x86_64):  MISSING"'
    - '[ -f coverage-debian.xml ] && echo "  Debian BM (x86_64):   OK" || echo "  Debian BM (x86_64):   MISSING"'
    - '[ -f coverage-ubuntu.xml ] && echo "  Ubuntu BM (x86_64):   OK" || echo "  Ubuntu BM (x86_64):   MISSING"'
    - '[ -d build-exotic ] && echo "  Exotic platforms:      artifacts present" || echo "  Exotic platforms:      no artifacts"'
    - echo "Pipeline $CI_PIPELINE_ID complete on $(date -u)"
  artifacts:
    paths:
      - coverage-k8s.xml
      - coverage-debian.xml
      - coverage-ubuntu.xml
    expire_in: 30 days
    when: always

# ============================================================
# Stage 3: Upload Artifacts to MinIO (buildsystem-artifacts)
# RBMM Path Hierarchy: {project}/{platform}/{arch}/{runner_class}/{pipeline_id}/
# ============================================================
upload-to-minio:
  stage: upload
  image: python:3.12-alpine
  tags:
    - kubernetes
  script:
    - echo "=== COMDARE CI v5.4.1 — MinIO Artifact Upload (S3v4 native) ==="
    - echo "RBMM Hierarchy — {project}/{platform}/{arch}/{runner_class}/{pipeline_id}/"
    - |
      python3 << 'PYEOF'
      import hashlib, hmac, datetime, os, sys, urllib.request, urllib.parse, urllib.error

      def get_signing_key(secret, datestamp, region, service):
          k = hmac.new(('AWS4' + secret).encode(), datestamp.encode(), hashlib.sha256).digest()
          k = hmac.new(k, region.encode(), hashlib.sha256).digest()
          k = hmac.new(k, service.encode(), hashlib.sha256).digest()
          return hmac.new(k, b'aws4_request', hashlib.sha256).digest()

      def s3_put(filepath, bucket, obj_key, endpoint, access, secret):
          with open(filepath, 'rb') as f:
              payload = f.read()
          parsed = urllib.parse.urlparse(endpoint)
          host = parsed.netloc
          scheme = parsed.scheme
          t = datetime.datetime.utcnow()
          amz_date = t.strftime('%Y%m%dT%H%M%SZ')
          ds = t.strftime('%Y%m%d')
          region, service = 'us-east-1', 's3'
          uri = '/' + bucket + '/' + obj_key
          payload_hash = hashlib.sha256(payload).hexdigest()
          headers_str = f'host:{host}\nx-amz-content-sha256:{payload_hash}\nx-amz-date:{amz_date}\n'
          signed_h = 'host;x-amz-content-sha256;x-amz-date'
          creq = f'PUT\n{uri}\n\n{headers_str}\n{signed_h}\n{payload_hash}'
          scope = f'{ds}/{region}/{service}/aws4_request'
          sts = f'AWS4-HMAC-SHA256\n{amz_date}\n{scope}\n{hashlib.sha256(creq.encode()).hexdigest()}'
          sig = hmac.new(get_signing_key(secret, ds, region, service), sts.encode(), hashlib.sha256).hexdigest()
          auth = f'AWS4-HMAC-SHA256 Credential={access}/{scope}, SignedHeaders={signed_h}, Signature={sig}'
          url = f'{scheme}://{host}{uri}'
          req = urllib.request.Request(url, data=payload, method='PUT')
          req.add_header('Authorization', auth)
          req.add_header('x-amz-date', amz_date)
          req.add_header('x-amz-content-sha256', payload_hash)
          req.add_header('Content-Type', 'application/xml')
          try:
              resp = urllib.request.urlopen(req, timeout=30)
              return resp.status
          except urllib.error.HTTPError as e:
              print(f'  HTTP {e.code}: {e.read().decode()[:100]}', file=sys.stderr)
              return e.code

      endpoint = os.environ.get('MINIO_ENDPOINT', '')
      access = os.environ.get('MINIO_ACCESS_KEY', '')
      secret = os.environ.get('MINIO_SECRET_KEY', '')
      proj = os.environ.get('CI_PROJECT_PATH', '')
      pid = os.environ.get('CI_PIPELINE_ID', '')
      bucket = 'buildsystem-artifacts'
      uploads = [
          ('coverage-k8s.xml', f'{proj}/linux/x86_64/k8s/{pid}/coverage/coverage-k8s.xml'),
          ('coverage-debian.xml', f'{proj}/linux/x86_64/debian/{pid}/coverage/coverage-debian.xml'),
          ('coverage-ubuntu.xml', f'{proj}/linux/x86_64/ubuntu/{pid}/coverage/coverage-ubuntu.xml'),
      ]
      ok = 0
      for local, key in uploads:
          if os.path.exists(local):
              code = s3_put(local, bucket, key, endpoint, access, secret)
              status = 'OK' if code == 200 else f'FAIL({code})'
              print(f'  {status} {key}')
              if code == 200: ok += 1
          else:
              print(f'  SKIP {local} (not found)')
      print(f'\nUpload complete: {ok}/{len(uploads)} files for pipeline {pid}')
      PYEOF
  allow_failure: true
  rules:
    - if: '$MINIO_ENDPOINT && $MINIO_ACCESS_KEY && $MINIO_SECRET_KEY'
      when: always
    - when: never

# =============================================================================
# PIPELINE 2: Real DB Experiment (sequential per-DB, 3 repetitions each)
# =============================================================================
# Runner: k8s-runner-4 (ID 17) on dedicated node talos-say-ls6
#   - Tag: "experiment" (only tag, run_untagged=false)
#   - Node selector: kubernetes.io/hostname=talos-say-ls6
#   - Toleration: experiment=dedicated:NoSchedule
# Safety: Samba AD lab user (dedup-lab), isolated lab schemas, cleanup guaranteed
# =============================================================================

.experiment-default: &experiment-default
  image: gcc:14-bookworm
  tags:
    - experiment
  before_script:
    - |
      apt-get update -qq > /dev/null 2>&1
      apt-get install -y -qq --no-install-recommends \
        libpq-dev libhiredis-dev librdkafka-dev libcurl4-openssl-dev \
        libmariadb-dev nlohmann-json3-dev cmake ninja-build nfs-common \
        > /dev/null 2>&1
      echo "Build-tools ready: GCC $(gcc -dumpversion) + CMake + Ninja"

experiment:build:
  <<: *experiment-default
  stage: experiment-build
  needs: []
  script:
    - |
      echo "=== Experiment Build: dedup-test v${CI_COMMIT_SHORT_SHA} ==="
      mkdir -p build && cd build
      cmake ../src/cpp -DCMAKE_BUILD_TYPE=Release
      make -j4 dedup-test 2>&1
      echo "=== Binary ready ==="
      ls -lh dedup-test
      file dedup-test
  artifacts:
    paths:
      - build/dedup-test
    expire_in: 7 days
  rules:
    - if: $CI_PIPELINE_SOURCE == "web" || $CI_PIPELINE_SOURCE == "api"
    - if: $CI_PIPELINE_SOURCE == "push"
      changes:
        - "src/cpp/**/*.cpp"
        - "src/cpp/**/*.hpp"
        - "src/cpp/**/*.h"
        - "src/cpp/CMakeLists.txt"
        - ".gitlab-ci.yml"

experiment:preflight:
  tags:
    - experiment
  image: gcc:14-bookworm
  stage: experiment-preflight
  needs: ["experiment:build"]
  timeout: 10 minutes
  retry:
    max: 2
    when: [runner_system_failure, script_failure, job_execution_timeout]
  script:
    - |
      echo "=== Pre-Flight: Checking all 7 DB systems (no apt needed) ==="
      PASS=0; FAIL=0
      tcp_check() {
        local host=$1 port=$2
        timeout 5 bash -c "echo > /dev/tcp/${host}/${port}" 2>/dev/null
      }
      http_check() {
        local url=$1
        timeout 5 bash -c "exec 3<>/dev/tcp/\$(echo $url | cut -d/ -f3 | cut -d: -f1)/\$(echo $url | cut -d: -f3 | cut -d/ -f1); echo -e 'GET /\$(echo $url | cut -d/ -f4-) HTTP/1.0\r\nHost: \$(echo $url | cut -d/ -f3 | cut -d: -f1)\r\n\r\n' >&3; head -1 <&3" 2>/dev/null | grep -q "200"
      }
      check() {
        if eval "$2" 2>/dev/null; then
          echo "  OK  $1"; PASS=$((PASS+1))
        else
          echo "  FAIL $1"; FAIL=$((FAIL+1))
        fi
      }
      check "PostgreSQL"   "tcp_check postgres-lb.databases.svc.cluster.local 5432"
      check "CockroachDB"  "tcp_check cockroachdb-public.cockroach-operator-system.svc.cluster.local 26257"
      check "MariaDB"      "tcp_check mariadb.databases.svc.cluster.local 3306"
      check "ClickHouse"   "tcp_check clickhouse.databases.svc.cluster.local 8123"
      check "Redis"        "tcp_check redis-cluster.redis.svc.cluster.local 6379"
      check "Kafka"        "tcp_check kafka-cluster-kafka-bootstrap.kafka.svc.cluster.local 9092"
      check "MinIO"        "tcp_check minio-lb.minio.svc.cluster.local 9000"
      echo "=== Result: ${PASS}/7 passed, ${FAIL}/7 failed ==="
      [ "$FAIL" -eq 0 ] || exit 1
  rules:
    - if: $CI_PIPELINE_SOURCE == "web" || $CI_PIPELINE_SOURCE == "api"
    - if: $CI_PIPELINE_SOURCE == "push"
      changes:
        - "src/cpp/**/*.cpp"
        - "src/cpp/**/*.hpp"
        - "src/cpp/**/*.h"
        - "src/cpp/CMakeLists.txt"
        - ".gitlab-ci.yml"

.experiment-per-db: &experiment-per-db
  <<: *experiment-default
  stage: experiment-run
  timeout: 6 hours
  retry:
    max: 1
    when: [runner_system_failure]
  script:
    - |
      echo "================================================================="
      echo "  EXPERIMENT: ${DB_SYSTEM} (3 repetitions)"
      echo "  Lab user: dedup-lab | Lab schema: dedup_lab"
      echo "  Payload types: ${PAYLOAD_TYPES}"
      echo "  Resume: checkpoint-dir + per-run-id tracking"
      echo "================================================================="

      # Try NFS mount for real-world datasets
      mkdir -p /datasets/real-world
      mount -t nfs4 10.0.110.184:/ /datasets/real-world 2>/dev/null && echo "[NFS] Mounted real-world datasets" || echo "[NFS] WARNING: Cannot mount 10.0.110.184 — NAS payload types will be skipped"

      chmod +x build/dedup-test
      OVERALL_EXIT=0

      for RUN in 1 2 3; do
        echo ""
        echo "===== ${DB_SYSTEM}: Attempt ${RUN}/3 ====="
        echo ""
        SEED=$((42 + RUN))
        mkdir -p results/${DB_SYSTEM}/run-${RUN}

        echo "===== Run ${RUN}/3 (seed=${SEED}) ====="

        build/dedup-test \
          --config src/cpp/config.example.json \
          --systems "${DB_SYSTEM}" \
          --generate-data \
          --data-dir /tmp/datasets \
          --results-dir "results/${DB_SYSTEM}/run-${RUN}" \
          --num-files 500 \
          --file-size 524288 \
          --seed ${SEED} \
          --checkpoint-dir "results/${DB_SYSTEM}/checkpoints" \
          --run-id ${RUN} \
          --max-retries 3 \
          --verbose 2>&1 || {
            echo "WARNING: Run ${RUN} exited with code $?"
            OVERALL_EXIT=1
          }

        echo "===== Run ${RUN}/3 complete ====="
        ls -la results/${DB_SYSTEM}/run-${RUN}/ 2>/dev/null || true
      done

      echo ""
      echo "=== All 3 runs complete for ${DB_SYSTEM} ==="
      echo "=== Overall exit: ${OVERALL_EXIT} ==="

      # Export combined results
      if ls results/${DB_SYSTEM}/run-*/combined_results.json 2>/dev/null; then
        echo "Results files found"
      fi

      umount /datasets/real-world 2>/dev/null || true
      exit ${OVERALL_EXIT}
  after_script:
    - |
      echo "=== Cleanup: dropping lab schemas ==="
      # Lab schema cleanup is handled by the binary's --cleanup flag
      # but we ensure NFS unmount here
      umount /datasets/real-world 2>/dev/null || true
  artifacts:
    paths:
      - results/
    expire_in: 365 days
    when: always
  rules:
    - if: $CI_PIPELINE_SOURCE == "web" || $CI_PIPELINE_SOURCE == "api"
    - if: $CI_PIPELINE_SOURCE == "push"
      changes:
        - "src/cpp/**/*.cpp"
        - "src/cpp/**/*.hpp"
        - "src/cpp/**/*.h"
        - "src/cpp/CMakeLists.txt"
        - ".gitlab-ci.yml"

experiment:postgresql:
  <<: *experiment-per-db
  needs: ["experiment:preflight", "experiment:build"]
  variables:
    DB_SYSTEM: "postgresql"
    PAYLOAD_TYPES: "random_binary,structured_json,text_document,uuid_keys,jsonb_documents,bank_transactions,text_corpus,numeric_dataset"

experiment:cockroachdb:
  <<: *experiment-per-db
  needs: ["experiment:postgresql", "experiment:build"]
  variables:
    DB_SYSTEM: "cockroachdb"
    PAYLOAD_TYPES: "random_binary,structured_json,text_document,uuid_keys,jsonb_documents,bank_transactions,text_corpus,numeric_dataset"

experiment:mariadb:
  <<: *experiment-per-db
  needs: ["experiment:cockroachdb", "experiment:build"]
  variables:
    DB_SYSTEM: "mariadb"
    PAYLOAD_TYPES: "random_binary,structured_json,text_document,uuid_keys,jsonb_documents,bank_transactions,text_corpus,numeric_dataset"

experiment:clickhouse:
  <<: *experiment-per-db
  needs: ["experiment:mariadb", "experiment:build"]
  variables:
    DB_SYSTEM: "clickhouse"
    PAYLOAD_TYPES: "random_binary,structured_json,text_document,uuid_keys,jsonb_documents,bank_transactions,text_corpus,numeric_dataset"

experiment:redis:
  <<: *experiment-per-db
  needs: ["experiment:clickhouse", "experiment:build"]
  variables:
    DB_SYSTEM: "redis"
    PAYLOAD_TYPES: "random_binary,structured_json,text_document,uuid_keys,jsonb_documents"

experiment:kafka:
  <<: *experiment-per-db
  needs: ["experiment:redis", "experiment:build"]
  variables:
    DB_SYSTEM: "kafka"
    PAYLOAD_TYPES: "random_binary,structured_json,text_document,uuid_keys,jsonb_documents"

experiment:minio:
  <<: *experiment-per-db
  needs: ["experiment:kafka", "experiment:build"]
  variables:
    DB_SYSTEM: "minio"
    PAYLOAD_TYPES: "random_binary,structured_json,text_document,uuid_keys,jsonb_documents"

experiment:run-all:
  stage: experiment-run
  tags:
    - experiment
  image: alpine:latest
  when: manual
  needs: ["experiment:build"]
  script:
    - echo "Manual trigger for full experiment cascade"
    - echo "Use individual DB jobs instead (they chain automatically)"
  rules:
    - if: $CI_PIPELINE_SOURCE == "web" || $CI_PIPELINE_SOURCE == "api"
      when: manual

experiment:cleanup:
  stage: experiment-cleanup
  tags:
    - experiment
  image: alpine:latest
  when: manual
  needs: []
  script:
    - echo "=== Post-Experiment Cleanup ==="
    - echo "Manual cleanup — verify results before running"
  rules:
    - if: $CI_PIPELINE_SOURCE == "web" || $CI_PIPELINE_SOURCE == "api"
      when: manual
